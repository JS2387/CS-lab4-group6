---
title: "lab4"
author: "Filip"
date: "11/26/2021"
output: html_document
---

We start out by finding the constant for this function. If we realize this is a Gamma function finding the constant is quite easy. Looking at $x^5e^{-x}, x>0$. If we have a look at the pdf for a Gamma PDF we have $f(x) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$. Looking at this we realise that for $\beta = 1$ and $\alpha=6$ we get $x^5e^{-x}$ and we now just need to find the constant multiplier.
Since we now know $\beta$ and $\alpha$ we can use the constant multiplier formula for a gamma function which we know to be $$\frac{\beta^{\alpha}}{\Gamma(\alpha)}$$ , by plugging in our values we get: $\frac{1^{6}}{\Gamma(6)}= \frac{1}{5!}$

This gives $f(x)=\frac{1}{5!} x^5e^{-x}$


### 1.1 
In this assignment we will be creating a Metropolis-Hastings algorithm. It works as follows.

First we select a initial $\theta_{0}$ value and then we do the following.
For $i=1,.....,n$ we draw a candiate value $\theta^{*}$~$g(\theta^{*}|\theta_{i-1})$. After this we calulate our $\alpha$ value.
\
$\alpha = \frac{g(\theta^{*})/g(\theta^{*}|\theta_{i-1}) }{g(\theta_{i-1})/g(\theta_{i-1}|\theta^{*})} = \frac{g(\theta^{*})g(\theta_{i-1}|\theta^{*}) }{g(\theta_{i-1}) g(\theta^{*}|\theta_{i-1})}$
\
If $\alpha \geq 1$ we accept the $\theta^{*}$ and set that to our current $\theta_i$. For $0 < \alpha <1$ we accept the $\theta$ and assign $\theta_i=\theta^{*}$ with the probability $\alpha$.If non of these happens we reject $\theta^{*}$ and we assign $\theta_i = \theta^{*}$ with the probability $1-\alpha$.
This is what our code below does.
\
Bellow our code can be seen. We first create a function that contains our targeted distrubution given in the assignment. Then we create the main function which takes three arguments , the numbers of steps , a starting point and probability P.The rest of the code does the things stated above. 

```{r}
library(ggplot2)
library(coda)
# Target distrubution function
pdf_function <- function(x) x^5*exp(-x)

# Initiate Metropolis-Hastings algorithm
Metro_hast <- function(steps ,starting_point, p ){
  #variable to keep track of our current point. 
  current_point <- starting_point
  
  #Loop to do our calulations
  for(i in 2:steps){
    currentX <- current_point[i-1]
    X_to_evaluate <- rlnorm(1, meanlog = log(currentX) , p)
    Uniform_var <- runif(1)
    
    ratio_numerator <- (pdf_function(X_to_evaluate)* dlnorm(currentX, meanlog = log(X_to_evaluate), p) )
    denominator <- (pdf_function(currentX)*dlnorm(X_to_evaluate,meanlog=log(X_to_evaluate), p))
    
    alpha <- min(1 , ratio_numerator / denominator)
    
    if(Uniform_var <= alpha)current_point[i] <- X_to_evaluate
    
    else current_point[i] <- currentX
  }
 # Return the points
  current_point
  
}

```

Now we want to visualize the data generated by our function.
```{r}
sample_1<-Metro_hast(2000,1,1)
df_1 <- as.data.frame(sample_1)
df_1$ID <- 1:2000

ggplot(df_1 , aes(x = ID , y = sample_1))+geom_line()

```
 
We set the numbers of steps to 2000, the starting point to one and the probability to one and get the plot above. Looking at the plot it does look like the mcmc is converging. 



##1.2 

```{r}

Metro_hast_chi <- function(steps ,starting_point ){
  #create our vectors 
  current_point <- rep(starting_point, steps)
  
  for(i in 2:steps) {
    
    currentX <- current_point[i-1]
    
    X_to_evaluate <- rchisq(1, df=floor(currentX +1))
    
    Uniform_var <- runif(1)
    
    ratio_numerator <- pdf_function(X_to_evaluate)*dchisq(currentX, df=floor(X_to_evaluate+1))
    
    denominator <- pdf_function(currentX)*dchisq(X_to_evaluate, df=floor(currentX+1))
    
    alpha <- min(1 , ratio_numerator / denominator)

    if(Uniform_var <= alpha)current_point[i] <- X_to_evaluate
    else current_point[i] <- currentX
    
    
    
  }
  current_point
  
  
}

sample2<-Metro_hast_chi(5000,rchisq(1,1))

df_2 <- as.data.frame(sample2)
df_2$ID <- 1:5000

ggplot(df_2 , aes(x = ID , y = sample2))+geom_line()

```

Above we create another Metropolis-Hastings algorithm that uses $\chi^2([X_t+1])$ as the proposal distrubution. The code works in the same way as our function in 1.1. Looking at the plot it does seem like this mcm also converges. 
### 1.3

```{r}
one_to_ten <- 1:10

Matrix_1<-matrix(,10,2000)
set.seed(1234)
for(i in 1:10){
  Matrix_1[one_to_ten,] <- Metro_hast_chi(2000, i )
}

Gelman_list <- list()

for (i in 1:10) {
  
  Gelman_list[[i]]<- as.mcmc(Matrix_1[i,])
  
}
gelman.diag(Gelman_list, confidence = 0.95, transform=FALSE, autoburnin=TRUE,
            multivariate=TRUE)



gelman.plot(Gelman_list, bin.width = 10, max.bins = 50,
            confidence = 0.95)
```
Above we see the Gelmanâ€“Rubin method and get a value of one which should be considered very good. This means that the variance within chains and between chains variance are equal. 
Looking at the plot showing a Gelman plot we can also see that the chains seems to converge after around 300 steps. After this point there is a few upticks but it seems to mostly converge. 



### 1.4 
We now want to investigate the following integral.
$\int_{0}^{\infty} xf(x) dx$ where $f(x)= \frac{1}{5!}x^5e^{-x}$. We can immediately realise that $\int_{0}^{\infty} f(x) dx = 1$ since integrating over a Probability density function gives us the cumulative distribution function which always equals one. We now set $g(x) = x$ and obtain the following expression.
$\int_{0}^{\infty} g(x)f(x)dx= E[g(X)]= \dfrac{1}{n}\sum_{i=1}^{n}E[X_i]=\frac{1}{n}\sum_{i=1}^{n}x_{i}$
\
We now calculate the mean from the data generated by our two functions.
```{r}
set.seed(1234)
draw_1 <-Metro_hast(100000,rlnorm(1,0,1),1)
mean(draw_1)

set.seed(1234)
draw_2 <- Metro_hast_chi(100000,rchisq(1,floor(1)))
mean(draw_2)
```
We can see here that both functions has a mean $\approx 6$ for a big sample. If we increase the size of the samples we should get even closer to 6. 

#### 1.5

The Gamma pdf is as follows: $f_x(x)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha -1 }e^{-\beta x}$
To find the expected value we do the following $E[X]=\int_{0}^{\infty}xf(x)dx= \frac{\beta^{\alpha}}{\Gamma(\alpha)}\int_{0}^{\infty}x^{{\alpha}}e^{-\beta x}dx$.
We now set $t= \beta x$ giving us a easier expression. $\frac{\beta^{\alpha}}{\Gamma(\alpha)} \int_{0}^{\infty} \left( \frac{t}{\beta}\right)^{\alpha}e^{-t}\frac{dt}{\beta}=\frac{\beta^{\alpha}}{\beta^{\alpha+1}\Gamma(\alpha)}\int_{0}^{\infty}t^{\alpha}e^{-t}dt = \frac{\Gamma(\alpha +1)}{\beta \Gamma(\alpha)}$ Here we now use the definition of the Gamma function to get: $\frac{\alpha\Gamma(\alpha)}{\beta \Gamma(\alpha)} = \frac{\alpha}{\beta}$  $\square$
\
Now we plug in your values for $\alpha$ and $\beta$ and get $\frac{6}{1}=6$.
This is in line with what we got in 1.4






